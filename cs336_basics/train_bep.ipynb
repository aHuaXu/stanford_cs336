{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:17:38.757391Z",
     "start_time": "2025-09-07T14:17:38.754081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import regex as re\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "61ed04b448c1cfe5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T14:20:09.805597Z",
     "start_time": "2025-09-07T14:20:09.796228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t = torch.arange(12, dtype=torch.float32).reshape((2, 2, -1))\n",
    "print(t)\n",
    "F.softmax(t, dim=1)"
   ],
   "id": "c6ed6a0a2f0aa5be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0474, 0.0474, 0.0474],\n",
       "         [0.9526, 0.9526, 0.9526]],\n",
       "\n",
       "        [[0.0474, 0.0474, 0.0474],\n",
       "         [0.9526, 0.9526, 0.9526]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "PAT = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "\n",
    "def init_vocab(special_tokens: list[str]) -> dict[int, bytes]:\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    for token in special_tokens:\n",
    "        bytes_token = token.encode(\"utf-8\")\n",
    "        vocab[len(vocab)] = bytes_token\n",
    "    return vocab\n",
    "\n",
    "def word2bytes(word: str) -> tuple[bytes, ...]:\n",
    "    a = list(word.encode(\"utf-8\"))\n",
    "    return tuple(bytes([b]) for b in a)"
   ],
   "id": "61a2d89666a6acff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_bpe(\n",
    "        input_path: str | os.PathLike,\n",
    "        vocab_size: int,\n",
    "        special_tokens: list[str],\n",
    "        **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    # 1. Vocab\n",
    "    # dict[int, bytes]\n",
    "    vocab: dict[int, bytes] = init_vocab(special_tokens)\n",
    "\n",
    "    # dict[tuple[bytes], int]\n",
    "    pre_token_counts: dict[tuple[bytes, ...], int] = {}\n",
    "\n",
    "    # 2. Pre-tokenization\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_processes = 4\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            f.seek(start)\n",
    "            chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            # remove special tokens\n",
    "            separator = \"|\".join([re.escape(token) for token in special_tokens])\n",
    "            separator_pat = re.compile(separator)\n",
    "            sub_chunks = separator_pat.split(chunk)\n",
    "            for sub_chunk in sub_chunks:\n",
    "                print(sub_chunk)\n",
    "\n",
    "            # Run pre-tokenization on your chunk and store the counts for each pre-token\n",
    "            for sub_chunk in sub_chunks:\n",
    "                for match in PAT.finditer(sub_chunk):\n",
    "                    token = match.group()\n",
    "                    if token in special_tokens:\n",
    "                        continue\n",
    "                    token_tuple = word2bytes(token)\n",
    "                    print(token, token_tuple)\n",
    "                    if len(token_tuple) < 2:\n",
    "                        continue\n",
    "                    pre_token_counts[token_tuple] = pre_token_counts.get(token_tuple, 0) + 1\n",
    "    print(\"pre_token_counts:{}\".format(pre_token_counts))\n",
    "\n",
    "    # 3. Merges\n",
    "    token_pairs_counts: dict[tuple[bytes, bytes], int] = {}\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    for pre_token, count in pre_token_counts.items():\n",
    "        for pair in zip(pre_token[:-1], pre_token[1:]):\n",
    "            # (bytes, bytes)\n",
    "            token_pairs_counts[pair] = token_pairs_counts.get(pair, 0) + count\n",
    "\n",
    "    # 4. Merges\n",
    "    while len(vocab) < vocab_size:\n",
    "        # get the most frequent pair, if there are multiple pairs with the same frequency, choose the lexicographically greater pair\n",
    "        sorted_token_pairs_counts = sorted(token_pairs_counts.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "        print(sorted_token_pairs_counts)\n",
    "        most_frequent_pair = sorted_token_pairs_counts[0][0]\n",
    "\n",
    "        merges.append(most_frequent_pair)\n",
    "        merged_token = most_frequent_pair[0] + most_frequent_pair[1]\n",
    "        vocab[len(vocab)] = merged_token\n",
    "        token_pairs_counts.pop(most_frequent_pair)\n",
    "\n",
    "        # replace the pre_token with the merged token\n",
    "        for pre_token, count in list(pre_token_counts.items()):\n",
    "            n = len(pre_token)\n",
    "            if not any(pre_token[i] == most_frequent_pair[0] and pre_token[i + 1] == most_frequent_pair[1] for i in\n",
    "                       range(n - 1)):\n",
    "                continue\n",
    "\n",
    "            # generate the new pre_token\n",
    "            new_pre_token: list[bytes] = []\n",
    "            i = 0\n",
    "            while i < n:\n",
    "                # match the most frequent pair\n",
    "                if i < n - 1 and pre_token[i] == most_frequent_pair[0] and pre_token[i + 1] == most_frequent_pair[1]:\n",
    "                    new_pre_token.append(merged_token)\n",
    "                    # update the prefix pair count\n",
    "                    if i > 0:\n",
    "                        old_pre_pair = (pre_token[i - 1], pre_token[i])\n",
    "                        token_pairs_counts[old_pre_pair] = token_pairs_counts[old_pre_pair] - count\n",
    "                        if token_pairs_counts[old_pre_pair] == 0:\n",
    "                            token_pairs_counts.pop(old_pre_pair)\n",
    "                    # update the suffix pair count\n",
    "                    if i < n - 2:\n",
    "                        old_suffix_pair = (pre_token[i + 1], pre_token[i + 2])\n",
    "                        token_pairs_counts[old_suffix_pair] = token_pairs_counts[old_suffix_pair] - count\n",
    "                        if token_pairs_counts[old_suffix_pair] == 0:\n",
    "                            token_pairs_counts.pop(old_suffix_pair)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_pre_token.append(pre_token[i])\n",
    "                    i += 1\n",
    "\n",
    "            # count the new_pre_token\n",
    "            for i in range(len(new_pre_token) - 1):\n",
    "                pair = (new_pre_token[i], new_pre_token[i + 1])\n",
    "                token_pairs_counts[pair] = token_pairs_counts.get(pair, 0) + count\n",
    "\n",
    "            # update the pre_token_counts with the new_pre_token\n",
    "            pre_token_counts[tuple(new_pre_token)] = count\n",
    "            pre_token_counts.pop(tuple(pre_token))\n",
    "        print(\"after merge pre_token_counts:{}\".format(pre_token_counts))\n",
    "\n",
    "    return vocab, merges"
   ],
   "id": "78502d65b5d6dcaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## for test",
   "id": "fd8b8512452224ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:52:37.299902Z",
     "start_time": "2025-08-27T15:52:37.292022Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low low low low low\r\n",
      "\n",
      "\r\n",
      "lower lower widest widest widest\r\n",
      "\n",
      "low (b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      "\r\n",
      " (b'\\r', b'\\n')\n",
      " (b'\\r',)\n",
      "\n",
      " (b'\\n',)\n",
      "lower (b'l', b'o', b'w', b'e', b'r')\n",
      " lower (b' ', b'l', b'o', b'w', b'e', b'r')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      "\r\n",
      " (b'\\r', b'\\n')\n",
      "\n",
      "\r\n",
      "newest newest newest newest newest newest\n",
      " (b'\\r',)\n",
      "\n",
      " (b'\\n',)\n",
      "newest (b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      "pre_token_counts:{(b'l', b'o', b'w'): 1, (b' ', b'l', b'o', b'w'): 4, (b'\\r', b'\\n'): 2, (b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'w', b'i', b'd', b'e', b's', b't'): 3, (b'n', b'e', b'w', b'e', b's', b't'): 1, (b' ', b'n', b'e', b'w', b'e', b's', b't'): 5}\n",
      "[((b's', b't'), 9), ((b'e', b's'), 9), ((b'w', b'e'), 8), ((b'o', b'w'), 7), ((b'l', b'o'), 7), ((b'n', b'e'), 6), ((b'e', b'w'), 6), ((b' ', b'n'), 5), ((b' ', b'l'), 5), ((b'w', b'i'), 3), ((b'i', b'd'), 3), ((b'd', b'e'), 3), ((b' ', b'w'), 3), ((b'e', b'r'), 2), ((b'\\r', b'\\n'), 2)]\n",
      "after merge pre_token_counts:{(b'l', b'o', b'w'): 1, (b' ', b'l', b'o', b'w'): 4, (b'\\r', b'\\n'): 2, (b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 1, (b' ', b'n', b'e', b'w', b'e', b'st'): 5}\n",
      "------\n",
      "low low low low low\n",
      "\n",
      "\n",
      "lower lower widest widest widest\n",
      "\n",
      "\n",
      "newest newest newest newest newest newest\n",
      "low (b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      " low (b' ', b'l', b'o', b'w')\n",
      "\n",
      " (b'\\n',)\n",
      "\n",
      " (b'\\n',)\n",
      "lower (b'l', b'o', b'w', b'e', b'r')\n",
      " lower (b' ', b'l', b'o', b'w', b'e', b'r')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      " widest (b' ', b'w', b'i', b'd', b'e', b's', b't')\n",
      "\n",
      " (b'\\n',)\n",
      "\n",
      " (b'\\n',)\n",
      "newest (b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      " newest (b' ', b'n', b'e', b'w', b'e', b's', b't')\n",
      "defaultdict(<class 'int'>, {(b'l', b'o', b'w'): 1, (b' ', b'l', b'o', b'w'): 4, (b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'w', b'i', b'd', b'e', b's', b't'): 3, (b'n', b'e', b'w', b'e', b's', b't'): 1, (b' ', b'n', b'e', b'w', b'e', b's', b't'): 5})\n",
      "defaultdict(<class 'int'>, {(b'l', b'o', b'w'): 1, (b' ', b'l', b'o', b'w'): 4, (b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'l', b'o', b'w', b'e', b'r'): 1, (b' ', b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 1, (b' ', b'n', b'e', b'w', b'e', b'st'): 5}) defaultdict(<class 'int'>, {(b'l', b'o'): 7, (b'o', b'w'): 7, (b' ', b'l'): 5, (b'w', b'e'): 8, (b'e', b'r'): 2, (b'n', b'e'): 6, (b'e', b'w'): 6, (b' ', b'w'): 3, (b'w', b'i'): 3, (b'i', b'd'): 3, (b'd', b'e'): 3, (b'e', b'st'): 9, (b' ', b'n'): 5})\n"
     ]
    }
   ],
   "execution_count": 39,
   "source": [
    "path = \"../data/test_bpe.txt\"\n",
    "vocab, merges = train_bpe(path, 258, [\"<|endoftext|>\"])"
   ],
   "id": "b0cf2c179f7ca808"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
